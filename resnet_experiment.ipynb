{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial PAMAP2 with mcfly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to get you familiar with training Neural Networks for time series using mcfly. At the end of the tutorial, you will have compared several Neural Network architectures you know how to train the best performing network.\n",
    "\n",
    "As an example dataset we use the publicly available [PAMAP2 dataset](https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring). It contains time series data from movement sensors worn by nine individuals. The data is labelled with the activity types that these individuals did and the aim is to train and evaluate a *classifier*.\n",
    "\n",
    "Before you can start, please make sure you install mcfly (see the [mcfly installation page](https://github.com/NLeSC/mcfly))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-64103e68aaef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# mcfly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmcfly\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodelgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfind_architecture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\mcfly\\mcfly\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmodelgen\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerate_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerate_DeepConvLSTM_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerate_CNN_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfind_architecture\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_models_on_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfind_best_architecture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkNN_accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\mcfly\\mcfly\\modelgen.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConvolution1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mConvolution2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# mcfly\n",
    "from mcfly import modelgen, find_architecture, storage\n",
    "from keras.models import load_model\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "from utils import tutorial_pamap2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data pre-procesed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a function for you to fetch the preprocessed data from https://zenodo.org/record/834467. Please specify the `directory_to_extract_to` in the code below and then execute the cell. This will download the preprocessed data into the directory in the `data` subdirectory. The output of the function is the path where the preprocessed data was stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify in which directory you want to store the data:\n",
    "directory_to_extract_to = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already downloaded and extracted.\n"
     ]
    }
   ],
   "source": [
    "data_path = tutorial_pamap2.download_preprocessed_data(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [PAMAP2 dataset](https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring) contains data from three movement sensors worn by nine test subjects. These subjects performed a protocol of several activities.\n",
    "\n",
    "The data originates from three sensors (on the hand, ankle and chest) and from each of the sensors we have three channels (acceleration on x, y and z axes). This gives us, for each time step, 9 values. The data is recorded on 100 Hz.\n",
    "\n",
    "The preprocessed data is split into smaller segments with a window of 512 time steps, corresponding to 5.12 seconds. We only include segments that completely fall into one activity period: the activity is the *label* of the segment.\n",
    "\n",
    "The goal of classification is to assign an activity label to an previously unseen segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the preprocessed data as stored in Numpy-files. Please note that the data has already been split up in a training (training), validation (val), and test subsets. It is common practice to call the input data X and the labels y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train_binary, X_val, y_val_binary, X_test, y_test_binary, labels = tutorial_pamap2.load_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data X and labels y are of type Numpy array. In the cell below we inspect the shape of the data. As you can see the shape of X is expressed as a Python tuple containing: the number of samples, length of the time series, and the number of channels for each sample. Similarly, the shape of y is represents the number of samples and the number of classes (unique labels). Note that y has the format of a binary array where only the correct class for each sample is assigned a 1. This is called one-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (11397, 512, 9)\n",
      "y shape: (11397, 7)\n"
     ]
    }
   ],
   "source": [
    "print('x shape:', X_train.shape)\n",
    "print('y shape:', y_train_binary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split between train test and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: (11397, 512, 9)\n",
      "validation set size: 100\n",
      "test set size: 1000\n"
     ]
    }
   ],
   "source": [
    "print('train set size:', X_train.shape)\n",
    "print('validation set size:', X_val.shape[0])\n",
    "print('test set size:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the distribution of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lying</th>\n",
       "      <td>0.136615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sitting</th>\n",
       "      <td>0.130736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standing</th>\n",
       "      <td>0.136703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walking</th>\n",
       "      <td>0.176625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cycling</th>\n",
       "      <td>0.118540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccuum_cleaning</th>\n",
       "      <td>0.125208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ironing</th>\n",
       "      <td>0.175573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  frequency\n",
       "lying              0.136615\n",
       "sitting            0.130736\n",
       "standing           0.136703\n",
       "walking            0.176625\n",
       "cycling            0.118540\n",
       "vaccuum_cleaning   0.125208\n",
       "ironing            0.175573"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = y_train_binary.mean(axis=0)\n",
    "frequencies_df = pd.DataFrame(frequencies, index=labels, columns=['frequency'])\n",
    "frequencies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question 1: How many channels does this dataset have?*\n",
    "### *Question 2: What is the least common activity label in this dataset?*\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step in the development of any deep learning model is to create a model architecture. As we do not know what architecture is best for our data we will create a set of random models to investigate which architecture is most suitable for our data and classification task. This process, creating random models, checking how good they are and then selecting the best one is called a 'random search'. A random search is considered to be the most robust approach to finding a good model. You will need to specificy how many models you want to create with argument 'number_of_models'. See for a full overview of the optional arguments the function documentation of modelgen.generate_models by running `modelgen.generate_models?`.\n",
    "\n",
    "##### What number of models to select?\n",
    "This number differs per dataset. More models will give better results but it will take longer to evaluate them. For the purpose of this tutorial we recommend trying only 2 models to begin with. If you have enough time you can try a larger number of models, e.g. 10 or 20 models. Because mcfly uses random search, you will get better results when using more models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_train_binary.shape[1]\n",
    "\n",
    "models = modelgen.generate_models(X_train.shape,\n",
    "                                  number_of_classes=num_classes,\n",
    "                                  number_of_models = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<keras.engine.sequential.Sequential at 0x7f84603dc940>,\n",
       "  {'learning_rate': 0.08360289270402858,\n",
       "   'regularization_rate': 0.0022439468517196116,\n",
       "   'filters': array([85, 17, 44]),\n",
       "   'fc_hidden_nodes': 443},\n",
       "  'CNN'),\n",
       " (<keras.engine.sequential.Sequential at 0x7f84adebfb70>,\n",
       "  {'learning_rate': 0.000893145093504032,\n",
       "   'regularization_rate': 0.00319386451934688,\n",
       "   'filters': [48, 43, 68, 77],\n",
       "   'lstm_dims': [78]},\n",
       "  'DeepConvLSTM')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 9)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 512, 64)      4672        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 512, 64)      256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 512, 64)      0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 512, 64)      32832       re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 512, 64)      256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 512, 64)      0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 512, 64)      32832       re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 512, 64)      256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 512, 64)      0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512, 73)      0           re_lu_3[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 512, 128)     46848       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 512, 128)     512         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 512, 128)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 512, 128)     82048       re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 512, 128)     512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 512, 128)     0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 512, 128)     82048       re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 512, 128)     512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 512, 128)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512, 201)     0           re_lu_6[0][0]                    \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 512, 128)     77312       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 512, 128)     512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 512, 128)     0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 512, 128)     49280       re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 512, 128)     512         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 512, 128)     0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 512, 128)     49280       re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 512, 128)     512         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 512, 128)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512, 329)     0           re_lu_9[0][0]                    \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 512, 7)       2310        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 7)            0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_1 (Softmax)             (None, 7)            0           global_average_pooling1d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 463,302\n",
      "Trainable params: 461,382\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christiaan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"so..., inputs=Tensor(\"in...)`\n"
     ]
    }
   ],
   "source": [
    "# Create resnet here (https://arxiv.org/pdf/1611.06455.pdf)\n",
    "from keras import Input\n",
    "from keras.layers import Convolution1D, Conv1D, BatchNormalization, ReLU, GlobalAvgPool1D, Softmax, Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "weightinit = 'lecun_uniform'\n",
    "regularization = 0\n",
    "\n",
    "x = Input(shape=(X_train.shape[1:]))\n",
    "inputs = x\n",
    "def conv_bn_relu_3_sandwich(x, filters, kernel_size):\n",
    "    first_x = x\n",
    "    for i in range(3):\n",
    "        x = Convolution1D(filters, kernel_size, padding='same', kernel_regularizer=l2(regularization))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)    \n",
    "    x = Concatenate()([x, first_x])\n",
    "    return x\n",
    "x = conv_bn_relu_3_sandwich(x, 64, 8)\n",
    "x = conv_bn_relu_3_sandwich(x, 128, 5)\n",
    "x = conv_bn_relu_3_sandwich(x, 128, 3)\n",
    "\n",
    "##### Layer is not in paper: ##################\n",
    "# Added to match filter number to label number\n",
    "x = Convolution1D(y_train_binary.shape[1],1, kernel_regularizer=l2(regularization))(x)\n",
    "###############################################\n",
    "\n",
    "x = GlobalAvgPool1D()(x)\n",
    "x = Softmax()(x)\n",
    "\n",
    "predictions = x\n",
    "\n",
    "outputs = x\n",
    "model = Model(input=inputs, outputs=predictions)\n",
    "wang_model = model\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the best model on the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified the best model architecture out of our random pool of models we can continue by training the model on the full training set.\n",
    "\n",
    "This would take some time, so instead we will train  on only a slightly larger subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/200\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 3.6300 - val_loss: 2.8957\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 2.6852 - val_loss: 2.1054\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9921 - val_loss: 1.5476\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 1.5572 - val_loss: 1.2160\n",
      "Epoch 5/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 1.2995 - val_loss: 0.9815\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 1.0078 - val_loss: 0.8186\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.8196 - val_loss: 0.7163\n",
      "Epoch 8/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.7796 - val_loss: 0.6419\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.6659 - val_loss: 0.5901\n",
      "Epoch 10/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.6082 - val_loss: 0.5509\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.5119 - val_loss: 0.5233\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.4786 - val_loss: 0.5054\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.4657 - val_loss: 0.4953\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.3735 - val_loss: 0.4915\n",
      "Epoch 15/200\n",
      "100/100 [==============================] - 3s 25ms/step - loss: 0.4098 - val_loss: 0.4842\n",
      "Epoch 16/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.3389 - val_loss: 0.4692\n",
      "Epoch 17/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.3460 - val_loss: 0.4495\n",
      "Epoch 18/200\n",
      "100/100 [==============================] - 3s 25ms/step - loss: 0.3252 - val_loss: 0.4411\n",
      "Epoch 19/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.2982 - val_loss: 0.4470\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.3476 - val_loss: 0.4623\n",
      "Epoch 21/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.3412 - val_loss: 0.4726\n",
      "Epoch 22/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.3779 - val_loss: 0.4954\n",
      "Epoch 23/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.3013 - val_loss: 0.5159\n",
      "Epoch 24/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.3337 - val_loss: 0.5066\n",
      "Epoch 25/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.2310 - val_loss: 0.5007\n",
      "Epoch 26/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.2120 - val_loss: 0.4994\n",
      "Epoch 27/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.2259 - val_loss: 0.5032\n",
      "Epoch 28/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.2448 - val_loss: 0.4964\n",
      "Epoch 29/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.2010 - val_loss: 0.4926\n",
      "Epoch 30/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.1851 - val_loss: 0.5044\n",
      "Epoch 31/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.2256 - val_loss: 0.5230\n",
      "Epoch 32/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.1848 - val_loss: 0.5377\n",
      "Epoch 33/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.1824 - val_loss: 0.5455\n",
      "Epoch 34/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.2240 - val_loss: 0.5449\n",
      "Epoch 35/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.2439 - val_loss: 0.5374\n",
      "Epoch 36/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.2736 - val_loss: 0.5458\n",
      "Epoch 37/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1642 - val_loss: 0.5673\n",
      "Epoch 38/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.1683 - val_loss: 0.5955\n",
      "Epoch 39/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.1529 - val_loss: 0.6087\n",
      "Epoch 40/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.2209 - val_loss: 0.6009\n",
      "Epoch 41/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.2359 - val_loss: 0.6257\n",
      "Epoch 42/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.1156 - val_loss: 0.6475\n",
      "Epoch 43/200\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.1495 - val_loss: 0.6583\n",
      "Epoch 44/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1715 - val_loss: 0.6941\n",
      "Epoch 45/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.1720 - val_loss: 0.7210\n",
      "Epoch 46/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.1917 - val_loss: 0.6896\n",
      "Epoch 47/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.1692 - val_loss: 0.6706\n",
      "Epoch 48/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.1142 - val_loss: 0.6740\n",
      "Epoch 49/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1209 - val_loss: 0.6816\n",
      "Epoch 50/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.1548 - val_loss: 0.6916\n",
      "Epoch 51/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.1189 - val_loss: 0.7242\n",
      "Epoch 52/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.1183 - val_loss: 0.7874\n",
      "Epoch 53/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1103 - val_loss: 0.8313\n",
      "Epoch 54/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0810 - val_loss: 0.8657\n",
      "Epoch 55/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.1428 - val_loss: 0.8972\n",
      "Epoch 56/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.1060 - val_loss: 0.9327\n",
      "Epoch 57/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0911 - val_loss: 0.9689\n",
      "Epoch 58/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.1009 - val_loss: 1.0159\n",
      "Epoch 59/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0722 - val_loss: 1.0475\n",
      "Epoch 60/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0819 - val_loss: 1.0602\n",
      "Epoch 61/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.2071 - val_loss: 1.0418\n",
      "Epoch 62/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0822 - val_loss: 1.0399\n",
      "Epoch 63/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.1029 - val_loss: 1.0543\n",
      "Epoch 64/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0652 - val_loss: 1.0709\n",
      "Epoch 65/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0956 - val_loss: 1.0845\n",
      "Epoch 66/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0955 - val_loss: 1.0930\n",
      "Epoch 67/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0904 - val_loss: 1.0837\n",
      "Epoch 68/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.1683 - val_loss: 1.0710\n",
      "Epoch 69/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.0854 - val_loss: 1.0386\n",
      "Epoch 70/200\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 0.1067 - val_loss: 1.0250\n",
      "Epoch 71/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.1202 - val_loss: 1.0223\n",
      "Epoch 72/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0774 - val_loss: 1.0098\n",
      "Epoch 73/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0890 - val_loss: 1.0089\n",
      "Epoch 74/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0795 - val_loss: 1.0189\n",
      "Epoch 75/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.1029 - val_loss: 1.0242\n",
      "Epoch 76/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0894 - val_loss: 1.0249\n",
      "Epoch 77/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1745 - val_loss: 1.0277\n",
      "Epoch 78/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0780 - val_loss: 1.0181\n",
      "Epoch 79/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0875 - val_loss: 1.0268\n",
      "Epoch 80/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0729 - val_loss: 1.0488\n",
      "Epoch 81/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0657 - val_loss: 1.0808\n",
      "Epoch 82/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0575 - val_loss: 1.0747\n",
      "Epoch 83/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0904 - val_loss: 1.0746\n",
      "Epoch 84/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.0716 - val_loss: 1.0768\n",
      "Epoch 85/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0570 - val_loss: 1.1053\n",
      "Epoch 86/200\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 0.0701 - val_loss: 1.1331\n",
      "Epoch 87/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0684 - val_loss: 1.1547\n",
      "Epoch 88/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0683 - val_loss: 1.1737\n",
      "Epoch 89/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0989 - val_loss: 1.1872\n",
      "Epoch 90/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0778 - val_loss: 1.1651\n",
      "Epoch 91/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0832 - val_loss: 1.2077\n",
      "Epoch 92/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.0704 - val_loss: 1.2839\n",
      "Epoch 93/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0563 - val_loss: 1.3117\n",
      "Epoch 94/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0342 - val_loss: 1.3370\n",
      "Epoch 95/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.1463 - val_loss: 1.3352\n",
      "Epoch 96/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.0424 - val_loss: 1.2908\n",
      "Epoch 97/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0608 - val_loss: 1.2906\n",
      "Epoch 98/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0534 - val_loss: 1.3196\n",
      "Epoch 99/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0572 - val_loss: 1.3415\n",
      "Epoch 100/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0775 - val_loss: 1.3394\n",
      "Epoch 101/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.0782 - val_loss: 1.3192\n",
      "Epoch 102/200\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 0.0484 - val_loss: 1.2754\n",
      "Epoch 103/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.0541 - val_loss: 1.2347\n",
      "Epoch 104/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.1085 - val_loss: 1.2031\n",
      "Epoch 105/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0877 - val_loss: 1.1904\n",
      "Epoch 106/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.1330 - val_loss: 1.1672\n",
      "Epoch 107/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0707 - val_loss: 1.1302\n",
      "Epoch 108/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.0715 - val_loss: 1.0825\n",
      "Epoch 109/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0856 - val_loss: 1.0664\n",
      "Epoch 110/200\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 0.0851 - val_loss: 1.0692\n",
      "Epoch 111/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0733 - val_loss: 1.0754\n",
      "Epoch 112/200\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 0.0920 - val_loss: 1.0568\n",
      "Epoch 113/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0731 - val_loss: 1.0115\n",
      "Epoch 114/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0734 - val_loss: 0.9971\n",
      "Epoch 115/200\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.0943 - val_loss: 0.9941\n",
      "Epoch 116/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.0590 - val_loss: 0.9978\n",
      "Epoch 117/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0547 - val_loss: 1.0108\n",
      "Epoch 118/200\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 0.0749 - val_loss: 1.0102\n",
      "Epoch 119/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0707 - val_loss: 1.0452\n",
      "Epoch 120/200\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.0420 - val_loss: 1.0921\n",
      "Epoch 121/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0585 - val_loss: 1.1474\n",
      "Epoch 122/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0650 - val_loss: 1.1754\n",
      "Epoch 123/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.1181 - val_loss: 1.1492\n",
      "Epoch 124/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.0854 - val_loss: 1.0238\n",
      "Epoch 125/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0410 - val_loss: 0.9367\n",
      "Epoch 126/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0639 - val_loss: 0.8865\n",
      "Epoch 127/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.0721 - val_loss: 0.8760\n",
      "Epoch 128/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0304 - val_loss: 0.8581\n",
      "Epoch 129/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0595 - val_loss: 0.8652\n",
      "Epoch 130/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0589 - val_loss: 0.8681\n",
      "Epoch 131/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0543 - val_loss: 0.8683\n",
      "Epoch 132/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0634 - val_loss: 0.8944\n",
      "Epoch 133/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0580 - val_loss: 0.9153\n",
      "Epoch 134/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.0863 - val_loss: 0.9524\n",
      "Epoch 135/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0775 - val_loss: 0.9851\n",
      "Epoch 136/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0605 - val_loss: 1.0223\n",
      "Epoch 137/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0472 - val_loss: 1.0445\n",
      "Epoch 138/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.1145 - val_loss: 1.0519\n",
      "Epoch 139/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0633 - val_loss: 1.0649\n",
      "Epoch 140/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0314 - val_loss: 1.0714\n",
      "Epoch 141/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0985 - val_loss: 1.0893\n",
      "Epoch 142/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.0300 - val_loss: 1.0955\n",
      "Epoch 143/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.0775 - val_loss: 1.1004\n",
      "Epoch 144/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0410 - val_loss: 1.0886\n",
      "Epoch 145/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0476 - val_loss: 1.1039\n",
      "Epoch 146/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0801 - val_loss: 1.1266\n",
      "Epoch 147/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.1854 - val_loss: 1.1295\n",
      "Epoch 148/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0450 - val_loss: 1.1316\n",
      "Epoch 149/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0383 - val_loss: 1.1230\n",
      "Epoch 150/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.0453 - val_loss: 1.1043\n",
      "Epoch 151/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0751 - val_loss: 1.0931\n",
      "Epoch 152/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0470 - val_loss: 1.1015\n",
      "Epoch 153/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.0417 - val_loss: 1.1183\n",
      "Epoch 154/200\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.0549 - val_loss: 1.1171\n",
      "Epoch 155/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.0521 - val_loss: 1.0851\n",
      "Epoch 156/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0554 - val_loss: 1.1413\n",
      "Epoch 157/200\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 0.0286 - val_loss: 1.1797\n",
      "Epoch 158/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.0434 - val_loss: 1.1984\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 35ms/step - loss: 0.0483 - val_loss: 1.2175\n",
      "Epoch 160/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.0708 - val_loss: 1.1917\n",
      "Epoch 161/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0405 - val_loss: 1.1510\n",
      "Epoch 162/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0555 - val_loss: 1.1019\n",
      "Epoch 163/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0864 - val_loss: 1.0844\n",
      "Epoch 164/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.1127 - val_loss: 1.1266\n",
      "Epoch 165/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0483 - val_loss: 1.1771\n",
      "Epoch 166/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1364 - val_loss: 1.1689\n",
      "Epoch 167/200\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 0.1178 - val_loss: 1.1329\n",
      "Epoch 168/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.1237 - val_loss: 1.1674\n",
      "Epoch 169/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0821 - val_loss: 1.1734\n",
      "Epoch 170/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0535 - val_loss: 1.1920\n",
      "Epoch 171/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0577 - val_loss: 1.1835\n",
      "Epoch 172/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0569 - val_loss: 1.1621\n",
      "Epoch 173/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.0273 - val_loss: 1.1346\n",
      "Epoch 174/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0654 - val_loss: 1.1169\n",
      "Epoch 175/200\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.0482 - val_loss: 1.1013\n",
      "Epoch 176/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0571 - val_loss: 1.0816\n",
      "Epoch 177/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0575 - val_loss: 1.0691\n",
      "Epoch 178/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0644 - val_loss: 1.0698\n",
      "Epoch 179/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.1282 - val_loss: 1.1262\n",
      "Epoch 180/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0293 - val_loss: 1.1580\n",
      "Epoch 181/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.0641 - val_loss: 1.1790\n",
      "Epoch 182/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0375 - val_loss: 1.2017\n",
      "Epoch 183/200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0877 - val_loss: 1.2202\n",
      "Epoch 184/200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.0786 - val_loss: 1.2645\n",
      "Epoch 185/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0324 - val_loss: 1.3107\n",
      "Epoch 186/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0477 - val_loss: 1.3427\n",
      "Epoch 187/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0262 - val_loss: 1.3755\n",
      "Epoch 188/200\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.0398 - val_loss: 1.4125\n",
      "Epoch 189/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0293 - val_loss: 1.4174\n",
      "Epoch 190/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0567 - val_loss: 1.4104\n",
      "Epoch 191/200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0341 - val_loss: 1.3654\n",
      "Epoch 192/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0250 - val_loss: 1.3409\n",
      "Epoch 193/200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0394 - val_loss: 1.3166\n",
      "Epoch 194/200\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.0340 - val_loss: 1.2790\n",
      "Epoch 195/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0648 - val_loss: 1.2629\n",
      "Epoch 196/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0288 - val_loss: 1.2895\n",
      "Epoch 197/200\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0409 - val_loss: 1.3133\n",
      "Epoch 198/200\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0195 - val_loss: 1.3248\n",
      "Epoch 199/200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0362 - val_loss: 1.3302\n",
      "Epoch 200/200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0173 - val_loss: 1.3322\n"
     ]
    }
   ],
   "source": [
    "#We make a copy of the model, to start training from fresh\n",
    "nr_epochs = 200\n",
    "datasize = 100 # Change in `X_train.shape[0]` if training complete data set\n",
    "history = wang_model.fit(X_train[:datasize,:,:], y_train_binary[:datasize,:],\n",
    "              epochs=nr_epochs, validation_data=(X_val, y_val_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss [2.8956511878967284, 2.1054317951202393, 1.547644352912903, 1.2159522342681885, 0.9815180206298828, 0.8186443471908569, 0.7163392972946167, 0.6418721461296082, 0.5900725603103638, 0.5509323930740356, 0.5232955050468445, 0.5054128432273864, 0.49534048318862917, 0.4915033483505249, 0.484206280708313, 0.46924701690673826, 0.4494542741775513, 0.4411033248901367, 0.44699023723602294, 0.4623290991783142, 0.4725939750671387, 0.49544501304626465, 0.5158901834487915, 0.5066285729408264, 0.5006854820251465, 0.4993626880645752, 0.5032028770446777, 0.4963810348510742, 0.4925814533233643, 0.5044474887847901, 0.5229779529571533, 0.5377362060546875, 0.5454844093322754, 0.5448711442947388, 0.5373769092559815, 0.5457673072814941, 0.567321138381958, 0.5954917430877685, 0.60868812084198, 0.6008945751190186, 0.6256803703308106, 0.6474853515625, 0.6583343887329102, 0.6940916967391968, 0.7210040664672852, 0.689592981338501, 0.6706331777572632, 0.6739588785171509, 0.681641960144043, 0.691551742553711, 0.7241780185699462, 0.7873672103881836, 0.8312627792358398, 0.8657390975952148, 0.8971803617477417, 0.9326530218124389, 0.9689165830612183, 1.0159280490875244, 1.0475192642211915, 1.0602337741851806, 1.0418121910095215, 1.0399139356613158, 1.0542861366271972, 1.070947256088257, 1.084489450454712, 1.0930445623397826, 1.0836764907836913, 1.0709723663330077, 1.038581099510193, 1.0249955606460572, 1.0222944402694703, 1.0098126745223999, 1.0088987684249877, 1.018861265182495, 1.024199423789978, 1.0248980283737184, 1.027684497833252, 1.0181001567840575, 1.0267564153671265, 1.0488207864761352, 1.0808229017257691, 1.0747407150268555, 1.0746197414398193, 1.076785545349121, 1.1052848243713378, 1.133100576400757, 1.1546962547302246, 1.1737111902236939, 1.1871672916412352, 1.1651039981842042, 1.207660846710205, 1.2839071893692016, 1.3116980600357055, 1.3370268392562865, 1.335151252746582, 1.2908026218414306, 1.2906215572357178, 1.319584574699402, 1.3415277433395385, 1.3393830633163453, 1.3191867542266846, 1.2754361820220947, 1.234729299545288, 1.2031155395507813, 1.19039306640625, 1.167188787460327, 1.1301764583587646, 1.0825469303131103, 1.0663926076889039, 1.0691626977920532, 1.0753708219528197, 1.0567839670181274, 1.0115029621124267, 0.997116231918335, 0.9941325902938842, 0.9977902507781983, 1.0108315134048462, 1.0101568412780761, 1.0452169466018677, 1.0920547485351562, 1.1473677444458008, 1.1754181098937988, 1.1491827297210693, 1.023803482055664, 0.9366966104507446, 0.8864721012115478, 0.8760277986526489, 0.8581302070617676, 0.8651542282104492, 0.8681321382522583, 0.8683485317230225, 0.8944400501251221, 0.9153378796577454, 0.9524379134178161, 0.985062928199768, 1.022279589176178, 1.0444891214370728, 1.0518911409378051, 1.0648607158660888, 1.0714036464691161, 1.089265956878662, 1.0955138683319092, 1.1003775691986084, 1.0885763120651246, 1.103878140449524, 1.1265851974487304, 1.1294832706451416, 1.131632561683655, 1.1230435609817504, 1.104345715045929, 1.0931322646141053, 1.101521031856537, 1.118320450782776, 1.1171249008178712, 1.0850856328010559, 1.141271367073059, 1.17968337059021, 1.198372359275818, 1.217544722557068, 1.191714506149292, 1.1509932684898376, 1.1018791675567627, 1.0843507480621337, 1.126587085723877, 1.177070655822754, 1.1689107894897461, 1.132854037284851, 1.1674077129364013, 1.1733972835540771, 1.1920229148864747, 1.183544864654541, 1.1621125888824464, 1.134623589515686, 1.1169156742095947, 1.1012605094909669, 1.0815514945983886, 1.0690669107437134, 1.0698406314849853, 1.1261634159088134, 1.158018741607666, 1.1789523029327393, 1.2017465782165528, 1.220212426185608, 1.2644650268554687, 1.3107402658462524, 1.3427377033233643, 1.3755394172668458, 1.412530927658081, 1.4174417448043823, 1.410356755256653, 1.3654444026947021, 1.3409124040603637, 1.3165983009338378, 1.2789520740509033, 1.262859697341919, 1.2895426654815674, 1.3133229541778564, 1.3247540664672852, 1.3301688098907472, 1.3321848106384278]\n",
      "loss [3.6300156021118166, 2.6851931953430177, 1.9920762443542481, 1.5571678352355958, 1.2995117902755737, 1.0078308153152467, 0.8196367549896241, 0.7796287822723389, 0.6658610439300537, 0.6082060432434082, 0.5118651866912842, 0.4785872173309326, 0.4656777667999268, 0.373543815612793, 0.409819643497467, 0.33886253356933593, 0.34600645065307617, 0.32515262603759765, 0.2982144212722778, 0.34761541843414306, 0.34121755123138425, 0.37788829326629636, 0.30125967025756833, 0.33373608827590945, 0.2309730839729309, 0.21202357053756715, 0.22594371795654297, 0.24480777263641357, 0.20103659152984618, 0.1850773286819458, 0.22563593864440917, 0.18483610153198243, 0.18241811990737916, 0.2240014600753784, 0.24394961833953857, 0.2736308526992798, 0.16424532413482665, 0.16834088802337646, 0.1529175329208374, 0.22093696117401124, 0.23590341329574585, 0.11560076475143433, 0.1494986128807068, 0.17145549297332763, 0.17199196577072143, 0.19174226760864257, 0.16917683839797973, 0.11415616750717163, 0.12087338149547577, 0.15480985403060912, 0.11889803171157837, 0.11831890344619751, 0.11027679085731507, 0.08102372050285339, 0.14281641244888305, 0.1060101580619812, 0.09112174987792969, 0.10094522953033447, 0.0721949028968811, 0.08191003620624543, 0.2071281623840332, 0.08216386318206786, 0.10291871905326844, 0.06518685460090637, 0.09564684212207794, 0.09548933088779449, 0.09042006969451905, 0.16833118677139283, 0.08537605345249176, 0.10673831343650818, 0.12022144317626954, 0.07738099634647369, 0.08901958107948303, 0.07950426459312439, 0.10286531448364258, 0.08936545610427857, 0.17447380661964418, 0.07797139942646027, 0.08745844602584839, 0.07293199062347412, 0.06567845940589905, 0.05749858677387237, 0.09036492586135864, 0.07155596137046814, 0.05704291701316833, 0.07006720542907714, 0.06840672969818115, 0.068277508020401, 0.09888796925544739, 0.07775073766708374, 0.0831928437948227, 0.0704131042957306, 0.056302568316459654, 0.03423820853233337, 0.1462823283672333, 0.04243119835853577, 0.06075914561748505, 0.053370510339736936, 0.05716402888298035, 0.07752558290958404, 0.07823329210281373, 0.04837787687778473, 0.05409885406494141, 0.10847072839736939, 0.08767106235027314, 0.13302218317985534, 0.07068607091903686, 0.07146063685417176, 0.08560151219367981, 0.08514997839927674, 0.07334069728851318, 0.09199661970138549, 0.0731272566318512, 0.07344540238380431, 0.09433637738227844, 0.05897347331047058, 0.0546704226732254, 0.07489465236663818, 0.07073810040950775, 0.04201461732387543, 0.05845305800437927, 0.064998619556427, 0.11811782360076904, 0.0854119598865509, 0.04104814022779465, 0.063857901096344, 0.07213767826557159, 0.030386473536491394, 0.05951482653617859, 0.05894220232963562, 0.05431493878364563, 0.06341296046972275, 0.057970940470695495, 0.08632593870162963, 0.07746851086616516, 0.060502816438674924, 0.04719662010669708, 0.11451690137386322, 0.06325772285461426, 0.031436005234718324, 0.098502037525177, 0.029952291697263718, 0.07750182688236236, 0.04100695729255676, 0.04755458056926727, 0.08011033415794372, 0.1854303425550461, 0.044996217489242554, 0.03830586194992065, 0.04527948260307312, 0.07506575226783753, 0.0470403003692627, 0.04174960851669311, 0.054867149591445924, 0.052121697068214415, 0.05542332947254181, 0.028605600595474245, 0.043431442379951474, 0.048333898186683655, 0.07082298338413238, 0.040509742498397824, 0.05553418636322022, 0.08638633489608764, 0.11270532727241517, 0.04830882489681244, 0.13640695452690124, 0.11776580214500428, 0.12368571758270264, 0.08213838338851928, 0.05347139179706573, 0.05768621861934662, 0.05692237675189972, 0.027280998826026918, 0.06543022155761719, 0.04818491250276566, 0.05713756799697876, 0.05750498116016388, 0.06442319512367249, 0.12815091729164124, 0.029338836073875427, 0.06411632597446441, 0.03749142825603485, 0.0876845920085907, 0.07859163045883179, 0.03236229956150055, 0.04770308077335358, 0.026175432801246644, 0.03978532910346985, 0.029301567673683165, 0.05669047296047211, 0.03409135818481445, 0.024973475933074953, 0.03943542063236236, 0.03403725624084473, 0.06483724236488342, 0.028752282708883286, 0.04089758932590484, 0.0195240980386734, 0.03620850145816803, 0.017298563793301584]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "for h in history.history:\n",
    "    print(h, history.history[h])\n",
    "    plt.plot(history.history[h])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question 7: Why does it take longer to train the best model for one epoch now than when we were  comparing model archicatures earlier on?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question 8: Do you think it is useful to train with more than 1 epoch?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving, loading and comparing reloaded model with original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be saved for future use. The savemodel function will save two separate files: a json file for the architecture and a npy (numpy array) file for the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelname = 'my_bestmodel.h5'\n",
    "model_path = os.path.join(resultpath,modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reloaded = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been reloaded. Let's reassure that this model has the same weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all([np.all(x==y) for x,y in zip(best_model.get_weights(), model_reloaded.get_weights())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now dive further into the Neural network that we created.\n",
    "We provide here a network that has been trained on the complete train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('./model/model.h5')\n",
    "model = wang_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the objects `models`, `best_model_fullytrained` and `best_model` that resulted from the mcfly functions are Keras objects. This means that you can use Keras functions on the objects, for example  `.predict`, (which when given the data, outputs the predictions for each sample) and `.evaluate` (which when given the data and the labels computes how well this model performs) . These functions are all documented in the [Keras documentation](https://keras.io/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect model predictions on validation data\n",
    "datasize = X_val.shape[0]\n",
    "probs = model.predict(X_val[:datasize,:,:],batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lying</th>\n",
       "      <th>sitting</th>\n",
       "      <th>standing</th>\n",
       "      <th>walking</th>\n",
       "      <th>cycling</th>\n",
       "      <th>vaccuum_cleaning</th>\n",
       "      <th>ironing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lying</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sitting</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walking</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cycling</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccuum_cleaning</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ironing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  lying  sitting  standing  walking  cycling  \\\n",
       "lying                13        0         0        0        0   \n",
       "sitting               0        5         1        0        8   \n",
       "standing              0        0        19        0        0   \n",
       "walking               0        0         0       15        0   \n",
       "cycling               0        0         0        1       11   \n",
       "vaccuum_cleaning      0        0         0        0        0   \n",
       "ironing               0        0         0        0        6   \n",
       "\n",
       "                  vaccuum_cleaning  ironing  \n",
       "lying                            0        0  \n",
       "sitting                          0        0  \n",
       "standing                         1        0  \n",
       "walking                          0        0  \n",
       "cycling                          0        0  \n",
       "vaccuum_cleaning                 8        0  \n",
       "ironing                          2       10  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#columns are predicted, rows are truth\n",
    "predicted = probs.argmax(axis=1)\n",
    "y_index = y_val_binary.argmax(axis=1)\n",
    "confusion_matrix = pd.crosstab(pd.Series(y_index), pd.Series(predicted))\n",
    "confusion_matrix.index = [labels[i] for i in confusion_matrix.index]\n",
    "confusion_matrix.columns = [labels[i] for i in confusion_matrix.columns]\n",
    "confusion_matrix.reindex(columns=[l for l in labels], fill_value=0)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 32s 32ms/step\n",
      "Score of best model: [2.3605376663208006, 0.503]\n"
     ]
    }
   ],
   "source": [
    "## Test on Testset\n",
    "score_test = models[0][0].evaluate(X_test, y_test_binary, verbose=True)\n",
    "print('Score of best model: ' + str(score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 7s 7ms/step\n",
      "Score of best model: 1.2170636038780211\n"
     ]
    }
   ],
   "source": [
    "score_test = wang_model.evaluate(X_test, y_test_binary, verbose=True)\n",
    "print('Score of best model: ' + str(score_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
